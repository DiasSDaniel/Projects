{"cells":[{"source":"# Data Analyst Associate Case Study Submission\n\n**You can use any tool that you want to do your analysis and create visualizations. Use this template to write up your summary for submission.**\n\nYou can use any markdown formatting you wish. If you are not familiar with Markdown, read the [Markdown Guide](https://s3.amazonaws.com/talent-assets.datacamp.com/Markdown+Guide.pdf) before you start.","metadata":{"tags":[]},"id":"a5b3bc0e-b993-4ee3-bd07-825a474bd0d9","cell_type":"markdown"},{"source":"## Data Validation\nDescribe the validation tasks you performed and what you found. Have you made any changes to the data to enable further analysis? Remember to describe what you did for every column in the data. \n\nAll variables were analyzed to ensure they matched the pre-established criteria. For each of the columns of the dataset, a specific data type was expected, as different types of information needed to be addressed differently. It was also needed to ensure the data fell into the expected range, as it may reveal errors in the data.\n### String variables\n- Variable 1 - “Claim ID”\nThis first variable is a unique identifier for each claim made against the company. The data type is correct and it is composed of 98 unique values, which was expected.\n- Variable 2 - “Location”\nThe data type is correct and its observations are divided into the four expected locations. The percentages of each location are as follows: SAO LUIS, 30.61%; RECIFE, 25.51%; FORTALEZA, 22.45%; NATAL, 21.43%.\n- Variable 3 - “Cause”\nNo correction on the data type was needed, but there were 78 values missing. These were replaced with the string 'unknown'. After this change, the variable presents 79.59% of observations as 'unknown'. The category 'meat' represents 12.25% of the data and 'vegetables' represents 8.16%.\n### Numeric variables\n- Variable 4 - “Time to Close”\nThe variable is classified into the correct type. Under further analysis there was one observation with a negative value, which is not possible as the measure is the number of days to close the claim. This observation also had a strange value for \"Individuals on Claim\", as the minimum expected for this variable is 1. As we don't know what kind of error was made and as there is only one observation with a negative value for \"Time to Close\", this observation was dropped from the dataset.\n- Variable 5 - \"Claim Amount\"\nThe variable was classified as a string object, so it must be changed to a numeric type. To do so, three methods were applied. First, the monetary symbol \"R$\" was stripped from all variables. Then, the thousands separators were removed, as it would disturb the data conversion. At least, the column was converted to 'float', a numeric type with decimals. No further changes were made to the column.\n- Variable 6 - \"Amount Paid\"\nThe column was correctly classified as a numeric type with a decimal part. The values are all in the plausible range for this variable.\n- Variable 7 - \"Individuals on Claim\"\nThe values of this column were correctly classified as integer numbers. However, there are 6 observations with the value of 0. To choose whether to drop these observations or to adjust them it was used cross evaluation. The values of \"Claim Amount\" and \"Amount Paid\" linked to these observations are low, their mean is slightly lower than the mean of the values linked with 1 individual on claim. As we see further, \"Individuals on Claim\" is strongly correlated with those variables, so low values indicate a low number of individuals. There was no other significant problem with those observations, so it was decided not to drop them, but to adjust their value of \"Individuals on Claim\" to the minimum expected.\n### Boolean variable\n- Variable 8 - \"Linked Cases\"\nThe variable is classified into the correct type. There are 9 observations with a value equal to True. One in RECIFE, 3 in SAO LUIS and 5 in NATAL. All of them have a high number of Individuals on Claim, between 15 and 41 (the highest in the data), as 75% of the data has an \"Individuals on Claim\" number equal to or below 6.\n\nAfter the analysis a dataset with 97 observations is left. ","metadata":{"tags":[]},"id":"d1b1a012-1ccb-40e7-a1ad-77b60b256690","cell_type":"markdown"},{"source":"## Data Discovery and Visualization\nDescribe what you found in the analysis and how the visualizations answer the customer questions in the project brief. In your description you should:\n- Include at least two different data visualizations to demonstrate the characteristics of variables\n- Include at least one data visualization to demonstrate the relationship between two or more variables\n- Describe how your analysis has answered the business questions in the project brief\n\n\nThe first aspect observed was the number of claims at each location. The claims are well distributed in this data, as the highest appearing location, SAO LUIS, represents a bit over 30% of the observations. The location with fewer claims is NATAL, which is a bit short of the 22% mark. The table below presents the number of claims per location and some other statistics.\n\n| Location | N° of Claims | Avg Amount Paid | Avg Individuals on Claim | N° of Linked Cases |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| **FORTALEZA** | **22** | R$ 39,592.80 | 3.14 | 0 |\n| **NATAL** | **21** | R$ 88,489.72 | 8.00 | 5 |\n| **RECIFE** | **25** | R$ 46,366.11 | 4.24 | 1 |\n| **SAO LUIS** | **29** | R$ 63,394.80 | 4.76 | 3 |\n\nWe can notice that NATAL has the highest average of Amount Paid. So, although that location has the least number of claims, it has the highest total Amount Paid of all four locations. Even though NATAL has fewer claims they are, on average, substantially more expensive.\n\n![Number of claims and Total amount paid by location](Graph_5.png)\n\nIt's also perceivable that NATAL has the highest average of individuals on claim. In fact, these two variables have a strong correlation. \n\n![Correlation Heatmap](Graph_4.png)\n\nAs expected, \"Claim Amount\" and \"Amount Paid\" also have a strong, positive correlation. The correlation between \"Claim Amount\" and \"Individuals on Claim\" is the strongest in the dataset. That happens because the amount claimed per individual is pretty standard among the data, with most observations ranging between 22000 and 33000. Time to Close has weak negative correlations with all the other variables.\n\nHere we see the number of cases divided by \"Cause\" in each location.\n\n![Cause distribution by location](Graph_2.png)\n\nWe see that they present a similar number of unknown causes, but SAO LUIS has 9 observations with known causes, surpassing RECIFE by four and the other two locations by six. ","metadata":{"tags":[]},"id":"714a7083-7a43-4b0b-8ed4-2675efc87691","cell_type":"markdown"},{"source":"Focusing now on the column \"Time to Close\", we can see how it's distributed across the data. The data for time to close is right-skewed, indicating that most claims are closed in a relative small number of days. In fact, half of the claims are closed in less than 640 days. However, if the claim isn't closed in this period, it could drag on for a longer time. This can be perceived by looking at the second bump in the graph, around 1795 days. It is also noted that a couple of observations pile up at the end of the left tail, showing that two claims took considerably more time to be closed. The average number of days to close a claim is 841.5, the minimum is 29 and the maximum 3591 (both in SAO LUIS). Curiously, all but three cases took a different number of days to be closed. These three took 120 days and don't appear to be connected as they are linked to different cities and present a false value for the variable 'Linked Cases'. \n\n![Distribution of time to close claims](Graph_1.png)","metadata":{},"id":"b9324da6-3cbc-4840-aeac-e10ea01ad930","cell_type":"markdown"},{"source":"Considering now how the time to closing a claim differs from location to location, some characteristics of the data are noticeable. The average time changes among locations. RECIFE and FORTALEZA have averages close to the dataset average, differing by 40 days (or one fifth of standard deviation) for more and for less, respectively. SAO LUIS has the highest average, of 1064 days, which is 243 days (or 1.2 standard deviation) higher than the dataset average. NATAL has an average 239 days (1.2 stds) lower than the dataset average and the lowest of the four locations, 581 days. We can also see that SAO LUIS has the most disperse data. Both the claims with fewest and most days to close are from this location. The data from NATAL, besides being the least dispersed, is also concentrated towards the bottom, as most of its claims took a relatively small number of days to close.\n\n![Boxplot of time to close claims for each Location](Graph_3.png)\n\nThe boxes above represent where are concentrated most of the observations of each location, with the bottom of the box representing the 25th quartile and the top representing the 75th quartile. Medians are represented by the lines in the middle of each box. The whiskers represent the variability outside the 25th and 75th quartiles, and values above or below them are considered outliers. These outliers push both NATAL and FORTALEZA's averages upwards, as we can see their medians are smaller.\n\nHistograms of time to close for each location show how the claims are distributed through the data. SAO LUIS' histogram presents sparse columns with gaps, besides observations with high values of time to close. The other three locations behave in a similar fashion to each other, when we observe the distribution of Time to Close. They present a dense concentration of relative low values with a secondary hill near 2000 days.  \n\n![Distribution of time to close claims for each Location](Graph_6.png)\n\n","metadata":{},"id":"5976322e-5648-4f70-8fdc-d709e0e02c81","cell_type":"markdown"},{"source":"## Conclusion\n\nAfter the analysis, it is possible to affirm that although the number of claims in the dataset is well divided between locations, the characteristics of those claims are different. NATAL has fewer claims, but they are, on average, more expensive. The variable \"Individuals on Claim\", with its strong correlation with \"Claim Amount\" and \"Amount Paid\", may be the reason for that, as the claims from NATAL have, on average, way more individuals involved.\n\nLooking towards the time to close a claim, the dataset presents some resemblance to a normal distribution skewed to the right. Claims that take a relatively long time to close are fewer and more sparse.\n\nComparing the different locations, NATAL is the one with the least variance of time to close, and it's also the one with the lower average and median for this variable. RECIFE and FORTALEZA are similar to NATAL, with right-skewed bell-shaped distributions. On the other hand, SAO LUIS presents the most variance, highest average and highest median, with a disperse distribution. ","metadata":{},"id":"876db480-7256-44e7-97ed-05ed89fef3e2","cell_type":"markdown"},{"source":"The code used to create the graphs and get the information used in this project is presented below, for consultation. ","metadata":{},"id":"72905348-3235-494d-8f8e-220ee6189205","cell_type":"markdown"},{"source":"\"\"\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport numpy as np\nimport seaborn as sns\n\nclaims = pd.read_csv('claims.csv')\n\nclaims.head()\nclaims.info()\nclaims.describe()\n\n# Claim ID\nlen(claims['Claim ID'].unique())\n\n# Location\nclaims['Location'].value_counts(normalize=True)\n                                \n# Cause\nclaims = claims.fillna('unknown')\nclaims['Cause'].value_counts(normalize=True)\n\n# Time to Close\nclaims[claims['Time to Close'] <= 0]\nclaims_clean = claims.drop(claims[claims['Time to Close'] <= 0].index)\n\n# Claim Amount\nclaims_clean['Claim Amount'] = claims_clean['Claim Amount'].str.strip('R$').str.replace(',','').astype(float)\nclaims_clean['Claim Amount'].describe()\n\n# Individuals on Claim\nclaims_clean['Individuals on Claim'].describe()\nclaims_clean.groupby('Individuals on Claim').describe().head()\nclaims_clean.loc[claims_clean['Individuals on Claim'] == 0, 'Individuals on Claim'] = 1\nclaims_clean.groupby('Individuals on Claim').describe().head()\n\n# Linked Cases\nclaims_clean['Linked Cases'].describe()\n\n# Number of claims per location\nclaims_clean.groupby('Location')['Claim ID'].count()\n\nclaims_clean.groupby('Location').mean()\n\nclaims_clean.groupby('Location')['Linked Cases'].sum()\n\nclaims_clean.corr()\n\n# Correlation heatmap\nmask = np.triu(np.ones_like(claims_clean[[ 'Claim Amount', 'Amount Paid', 'Individuals on Claim', 'Time to Close']].corr(), dtype=np.bool))\nfig , ax = plt.subplots()\nfig.set_figwidth(4)\nfig.set_figheight(2)\nsns.heatmap(claims_clean[[ 'Claim Amount', 'Amount Paid', 'Individuals on Claim', 'Time to Close']].corr(),vmax=1, vmin=-1,annot=True, mask=mask,  fmt='.3f', cmap='BuPu', annot_kws= {'color':'white'})\nplt.xticks(rotation=45, ha='right')\nplt.title('Correlation heatmap')\nplt.show()\n\n# Claim Amount vs Individuals on Claim\nclaims_clean['claim_amount_per_ind'] = claims_clean['Claim Amount']/claims_clean['Individuals on Claim']\nclaims_clean['claim_amount_per_ind'].describe()\n\n#Extra graph\nfig, ax = plt.subplots()\nax2 = ax.twinx()\nsns.histplot(data=claims_clean, x='claim_amount_per_ind', bins='fd', ax=ax)\nsns.kdeplot(data=claims_clean, x='claim_amount_per_ind', hue='Location', ax=ax2)\nplt.show()\n\n# Number of claims vs Total Amount paid\nsns.set_palette('deep')\nsum_amount = claims_clean.groupby('Location')['Amount Paid'].sum()/1000000\nnum_claims = claims_clean.groupby('Location')['Claim ID'].count()\n\nfig, ax = plt.subplots()\nax2 = ax.twinx()\nnum_claims.plot(kind='bar', color='indianred', ax=ax, width=0.2, position=1, label='Number of claims')\nsum_amount.plot(kind='bar', color='lightsteelblue', ax=ax2, width=0.2, position=0, label='Total Amount in Million R$')\nr = np.arange(len(sum_amount))\nplt.xticks(r, ['FORTALEZA', 'NATAL', 'RECIFE', 'SAO LUIS'])\nax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=0, ha='center')\nfig.legend( bbox_to_anchor=(1.35,0.25))\nplt.title('Number of claims and Total amount paid by location')\nplt.show()\n\n# Cause per location\nclaims_clean.groupby('Location')['Cause'].value_counts(normalize=True)\nclaims_clean.groupby('Location')['Cause'].value_counts()\n\nsns.set_palette('deep')\ncause_by_loc = claims_clean.groupby('Location')['Cause'].value_counts().sort_index()\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\nlabel = ['meat','unknown', 'vegetables']\nax1.pie(cause_by_loc['FORTALEZA'], autopct=lambda p: '{:.0f}'.format(p * cause_by_loc['FORTALEZA'].sum() / 100) )\nax1.set_title('FORTALEZA', fontdict={'fontsize': 11})\nax2.pie(cause_by_loc['NATAL'], autopct=lambda p: '{:.0f}'.format(p * cause_by_loc['NATAL'].sum() / 100))\nax2.set_title('NATAL', fontdict={'fontsize': 11})\nax3.pie(cause_by_loc['RECIFE'], autopct=lambda p: '{:.0f}'.format(p * cause_by_loc['RECIFE'].sum() / 100))\nax3.set_title('RECIFE', fontdict={'fontsize': 11})\nax4.pie(cause_by_loc['SAO LUIS'], autopct=lambda p: '{:.0f}'.format(p * cause_by_loc['SAO LUIS'].sum() / 100))\nax4.set_title('SAO LUIS', fontdict={'fontsize': 11})\nax4.legend(labels=label, bbox_to_anchor=(1.1,1))\nplt.show()\n\n# Distribution of time to close\nclaims_clean['Time to Close'].describe()\n\nclaims_clean['Time to Close'].sort_values().value_counts().head()\n\nclaims_clean[claims_clean['Time to Close'] ==120]\n\nmean_time = claims_clean['Time to Close'].mean\nprint(mean_time)\nfig,ax=plt.subplots()\nsns.histplot(data=claims_clean, x='Time to Close', bins='fd', kde=True, ax=ax)\nplt.axvline(mean_time, 0, 28, color='orange', label=\"Average days to close claim\", linestyle='--')\nplt.xlabel(\"Days to close the claim\")\nplt.ylabel(\"Number of claims\")\nplt.title(\"Distribution of time to close claims\")\nplt.legend()\nplt.show()\n\n# Distribution of time to close on each location\nsns.catplot(data=claims_clean, y='Time to Close', x=\"Location\", kind='box', showmeans=True, meanprops={'marker':'o','mec':'orange', 'mfc': 'orangered'},  order=['FORTALEZA', 'NATAL', 'RECIFE', 'SAO LUIS'])\nplt.title(\"Distribution of time to close claims for each Location\")\nmean_label = mlines.Line2D([], [], color='orangered', marker='o', markeredgecolor='orange', linestyle='None', markersize=6, label='Average days to close')\nplt.legend(handles=[mean_label])\nplt.ylabel('Days to close claim')\nplt.show()\n\ng = sns.displot(data=claims_clean, x='Time to Close', col=\"Location\", kde=True, col_order=['FORTALEZA', 'NATAL', 'RECIFE', 'SAO LUIS'])\ng.set_axis_labels(\"Time to close claim\", \"Number of claims\")\ng.set_titles(\"{col_name}\")\nplt.show()\n\n\n\n\"\"\"","metadata":{},"id":"7862c0aa-8cae-4014-882b-4ba9acd0cdbb","cell_type":"code","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport matplotlib.lines as mlines\\nimport numpy as np\\nimport seaborn as sns\\n\\nclaims = pd.read_csv(\\'claims.csv\\')\\n\\nclaims.head()\\nclaims.info()\\nclaims.describe()\\n\\n# Claim ID\\nlen(claims[\\'Claim ID\\'].unique())\\n\\n# Location\\nclaims[\\'Location\\'].value_counts(normalize=True)\\n                                \\n# Cause\\nclaims = claims.fillna(\\'unknown\\')\\nclaims[\\'Cause\\'].value_counts(normalize=True)\\n\\n# Time to Close\\nclaims[claims[\\'Time to Close\\'] <= 0]\\nclaims_clean = claims.drop(claims[claims[\\'Time to Close\\'] <= 0].index)\\n\\n# Claim Amount\\nclaims_clean[\\'Claim Amount\\'] = claims_clean[\\'Claim Amount\\'].str.strip(\\'R$\\').str.replace(\\',\\',\\'\\').astype(float)\\nclaims_clean[\\'Claim Amount\\'].describe()\\n\\n# Individuals on Claim\\nclaims_clean[\\'Individuals on Claim\\'].describe()\\nclaims_clean.groupby(\\'Individuals on Claim\\').describe().head()\\nclaims_clean.loc[claims_clean[\\'Individuals on Claim\\'] == 0, \\'Individuals on Claim\\'] = 1\\nclaims_clean.groupby(\\'Individuals on Claim\\').describe().head()\\n\\n# Linked Cases\\nclaims_clean[\\'Linked Cases\\'].describe()\\n\\n# Number of claims per location\\nclaims_clean.groupby(\\'Location\\')[\\'Claim ID\\'].count()\\n\\nclaims_clean.groupby(\\'Location\\').mean()\\n\\nclaims_clean.groupby(\\'Location\\')[\\'Linked Cases\\'].sum()\\n\\nclaims_clean.corr()\\n\\n# Correlation heatmap\\nmask = np.triu(np.ones_like(claims_clean[[ \\'Claim Amount\\', \\'Amount Paid\\', \\'Individuals on Claim\\', \\'Time to Close\\']].corr(), dtype=np.bool))\\nfig , ax = plt.subplots()\\nfig.set_figwidth(4)\\nfig.set_figheight(2)\\nsns.heatmap(claims_clean[[ \\'Claim Amount\\', \\'Amount Paid\\', \\'Individuals on Claim\\', \\'Time to Close\\']].corr(),vmax=1, vmin=-1,annot=True, mask=mask,  fmt=\\'.3f\\', cmap=\\'BuPu\\', annot_kws= {\\'color\\':\\'white\\'})\\nplt.xticks(rotation=45, ha=\\'right\\')\\nplt.title(\\'Correlation heatmap\\')\\nplt.show()\\n\\n# Claim Amount vs Individuals on Claim\\nclaims_clean[\\'claim_amount_per_ind\\'] = claims_clean[\\'Claim Amount\\']/claims_clean[\\'Individuals on Claim\\']\\nclaims_clean[\\'claim_amount_per_ind\\'].describe()\\n\\n#Extra graph\\nfig, ax = plt.subplots()\\nax2 = ax.twinx()\\nsns.histplot(data=claims_clean, x=\\'claim_amount_per_ind\\', bins=\\'fd\\', ax=ax)\\nsns.kdeplot(data=claims_clean, x=\\'claim_amount_per_ind\\', hue=\\'Location\\', ax=ax2)\\nplt.show()\\n\\n# Number of claims vs Total Amount paid\\nsns.set_palette(\\'deep\\')\\nsum_amount = claims_clean.groupby(\\'Location\\')[\\'Amount Paid\\'].sum()/1000000\\nnum_claims = claims_clean.groupby(\\'Location\\')[\\'Claim ID\\'].count()\\n\\nfig, ax = plt.subplots()\\nax2 = ax.twinx()\\nnum_claims.plot(kind=\\'bar\\', color=\\'indianred\\', ax=ax, width=0.2, position=1, label=\\'Number of claims\\')\\nsum_amount.plot(kind=\\'bar\\', color=\\'lightsteelblue\\', ax=ax2, width=0.2, position=0, label=\\'Total Amount in Million R$\\')\\nr = np.arange(len(sum_amount))\\nplt.xticks(r, [\\'FORTALEZA\\', \\'NATAL\\', \\'RECIFE\\', \\'SAO LUIS\\'])\\nax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=0, ha=\\'center\\')\\nfig.legend( bbox_to_anchor=(1.35,0.25))\\nplt.title(\\'Number of claims and Total amount paid by location\\')\\nplt.show()\\n\\n# Cause per location\\nclaims_clean.groupby(\\'Location\\')[\\'Cause\\'].value_counts(normalize=True)\\nclaims_clean.groupby(\\'Location\\')[\\'Cause\\'].value_counts()\\n\\nsns.set_palette(\\'deep\\')\\ncause_by_loc = claims_clean.groupby(\\'Location\\')[\\'Cause\\'].value_counts().sort_index()\\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\\nlabel = [\\'meat\\',\\'unknown\\', \\'vegetables\\']\\nax1.pie(cause_by_loc[\\'FORTALEZA\\'], autopct=lambda p: \\'{:.0f}\\'.format(p * cause_by_loc[\\'FORTALEZA\\'].sum() / 100) )\\nax1.set_title(\\'FORTALEZA\\', fontdict={\\'fontsize\\': 11})\\nax2.pie(cause_by_loc[\\'NATAL\\'], autopct=lambda p: \\'{:.0f}\\'.format(p * cause_by_loc[\\'NATAL\\'].sum() / 100))\\nax2.set_title(\\'NATAL\\', fontdict={\\'fontsize\\': 11})\\nax3.pie(cause_by_loc[\\'RECIFE\\'], autopct=lambda p: \\'{:.0f}\\'.format(p * cause_by_loc[\\'RECIFE\\'].sum() / 100))\\nax3.set_title(\\'RECIFE\\', fontdict={\\'fontsize\\': 11})\\nax4.pie(cause_by_loc[\\'SAO LUIS\\'], autopct=lambda p: \\'{:.0f}\\'.format(p * cause_by_loc[\\'SAO LUIS\\'].sum() / 100))\\nax4.set_title(\\'SAO LUIS\\', fontdict={\\'fontsize\\': 11})\\nax4.legend(labels=label, bbox_to_anchor=(1.1,1))\\nplt.show()\\n\\n# Distribution of time to close\\nclaims_clean[\\'Time to Close\\'].describe()\\n\\nclaims_clean[\\'Time to Close\\'].sort_values().value_counts().head()\\n\\nclaims_clean[claims_clean[\\'Time to Close\\'] ==120]\\n\\nmean_time = claims_clean[\\'Time to Close\\'].mean\\nprint(mean_time)\\nfig,ax=plt.subplots()\\nsns.histplot(data=claims_clean, x=\\'Time to Close\\', bins=\\'fd\\', kde=True, ax=ax)\\nplt.axvline(mean_time, 0, 28, color=\\'orange\\', label=\"Average days to close claim\", linestyle=\\'--\\')\\nplt.xlabel(\"Days to close the claim\")\\nplt.ylabel(\"Number of claims\")\\nplt.title(\"Distribution of time to close claims\")\\nplt.legend()\\nplt.show()\\n\\n# Distribution of time to close on each location\\nsns.catplot(data=claims_clean, y=\\'Time to Close\\', x=\"Location\", kind=\\'box\\', showmeans=True, meanprops={\\'marker\\':\\'o\\',\\'mec\\':\\'orange\\', \\'mfc\\': \\'orangered\\'},  order=[\\'FORTALEZA\\', \\'NATAL\\', \\'RECIFE\\', \\'SAO LUIS\\'])\\nplt.title(\"Distribution of time to close claims for each Location\")\\nmean_label = mlines.Line2D([], [], color=\\'orangered\\', marker=\\'o\\', markeredgecolor=\\'orange\\', linestyle=\\'None\\', markersize=6, label=\\'Average days to close\\')\\nplt.legend(handles=[mean_label])\\nplt.ylabel(\\'Days to close claim\\')\\nplt.show()\\n\\ng = sns.displot(data=claims_clean, x=\\'Time to Close\\', col=\"Location\", kde=True, col_order=[\\'FORTALEZA\\', \\'NATAL\\', \\'RECIFE\\', \\'SAO LUIS\\'])\\ng.set_axis_labels(\"Time to close claim\", \"Number of claims\")\\ng.set_titles(\"{col_name}\")\\nplt.show()\\n\\n\\n\\n'"},"metadata":{}}]},{"source":"## ✅ When you have finished...\n-  Publish your Workspace using the option on the left\n-  Check the published version of your report:\n\t-  Can you see everything you want us to grade?\n    -  Are all the graphics visible?\n-  Review the grading rubric, have you included everything that will be graded?\n-  Head back to the [Certification Dashboard](https://app.datacamp.com/certification) to submit your case study","metadata":{"tags":[]},"id":"f635131a-00f8-44dd-b610-509db41c1154","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}